---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: rook-ceph-cluster
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default
  sources:
    - chart: rook-ceph-cluster
      repoURL: https://charts.rook.io/release
      targetRevision: v1.18.9
      helm:
        valuesObject:
          monitoring:
            enabled: true
            createPrometheusRules: true
          toolbox:
            enabled: true
          cephClusterSpec:
            cephConfig:
              global:
                mon_max_pg_per_osd: "400"
            dashboard:
              enabled: true
              urlPrefix: /
              ssl: false
              prometheusEndpoint: http://prometheus-operated.monitoring.svc.cluster.local:9090
            crashCollector:
              disable: false
            csi:
              readAffinity:
                enabled: true
              cephfs:
                kernelMountOptions: "ms_mode=prefer-crc"
            mgr:
              modules:
                - name: pg_autoscaler
                  enabled: true
            network:
              provider: host
              addressRanges:
                public: ["10.0.0.0/16"]
                cluster: ["169.254.255.0/24"]
              connections:
                requireMsgr2: true
            storage:
              useAllNodes: true
              useAllDevices: false
              devicePathFilter: /dev/disk/by-id/nvme-SAMSUNG_MZQL21T9HCJR-00B7C_.*
          cephBlockPools:
            - name: ceph-blockpool
              spec:
                failureDomain: host
                replicated:
                  size: 3
              storageClass:
                enabled: true
                name: ceph-block
                isDefault: false
                reclaimPolicy: Delete
                allowVolumeExpansion: true
                parameters:
                  imageFormat: "2"
                  imageFeatures: layering,exclusive-lock,object-map,fast-diff,deep-flatten
                  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
                  csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
                  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
                  csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
                  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
                  csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
                  csi.storage.k8s.io/fstype: ext4
            - name: ceph-blockpool-xfs
              spec:
                failureDomain: host
                replicated:
                  size: 3
              storageClass:
                enabled: true
                name: ceph-block-xfs
                isDefault: true
                reclaimPolicy: Delete
                allowVolumeExpansion: true
                parameters:
                  imageFormat: "2"
                  imageFeatures: layering,exclusive-lock,object-map,fast-diff,deep-flatten
                  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
                  csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
                  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
                  csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
                  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
                  csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
                  csi.storage.k8s.io/fstype: xfs
          cephBlockPoolsVolumeSnapshotClass:
            enabled: true
            name: csi-ceph-block
            isDefault: true
            deletionPolicy: Delete
            labels:
              velero.io/csi-volumesnapshot-class: "true"
          cephFileSystems:
            - name: ceph-filesystem
              spec:
                metadataPool:
                  replicated:
                    size: 3
                dataPools:
                  - failureDomain: host
                    replicated:
                      size: 3
                    name: data0
                metadataServer:
                  activeCount: 1
                  activeStandby: true
                  resources:
                    limits:
                      memory: "4Gi"
                    requests:
                      cpu: '1'
                      memory: "4Gi"
                  priorityClassName: system-cluster-critical
              storageClass:
                enabled: true
                isDefault: false
                name: ceph-filesystem
                pool: data0
                reclaimPolicy: Delete
                allowVolumeExpansion: true
                volumeBindingMode: "Immediate"
                annotations: {}
                labels: {}
                mountOptions: []
                parameters:
                  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
                  csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
                  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
                  csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
                  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
                  csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
                  csi.storage.k8s.io/fstype: xfs
          cephFileSystemVolumeSnapshotClass:
            enabled: true
            name: ceph-filesystem
            isDefault: false
            deletionPolicy: Delete
            labels:
              velero.io/csi-volumesnapshot-class: "true"
          cephObjectStores:
            - name: ceph-objectstore
              storageClass:
                enabled: true
                name: ceph-objectstore
              spec:
                allowUsersInNamespaces:
                  - "*"
                metadataPool:
                  failureDomain: host
                  replicated:
                    size: 3
                dataPool:
                  failureDomain: host
                  replicated:
                    size: 3
                gateway:
                  port: 80
                  instances: 2
                hosting:
                  dnsNames:
                    - &host s3.k8s-01.suslian.engineer
  destination:
    name: in-cluster
    namespace: rook-ceph
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
